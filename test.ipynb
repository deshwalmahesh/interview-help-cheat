{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5 (consumer):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/cw/t5b5dh315pb4f2w38_466zs80000gn/T/ipykernel_50748/1122364220.py\", line 91, in consumer\n",
      "AttributeError: 'Queue' object has no attribute '_queue'. Did you mean: 'queue'?\n"
     ]
    }
   ],
   "source": [
    "import queue\n",
    "import re\n",
    "import pyaudio\n",
    "import asyncio\n",
    "import torch \n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import numpy as np\n",
    "import threading\n",
    "\n",
    "\n",
    "# Audio settings\n",
    "STEP_IN_SEC: int = 1 # We'll increase the processable audio data by this\n",
    "LENGTH_IN_SEC: int = 6 #  MAximum time duration at which the audio data will pe processed together at once. Think of it as sliding window\n",
    "NB_CHANNELS = 1 # Input channels. For mic, it's usually 1\n",
    "SAMPLE_RATE = 16000 # Per second Sampling Rate\n",
    "CHUNK = SAMPLE_RATE\n",
    "\n",
    "# Queues\n",
    "audio_queue = queue.Queue()\n",
    "length_queue = queue.Queue(maxsize=LENGTH_IN_SEC)\n",
    "\n",
    "# Whisper settings\n",
    "LANGUAGE = \"english\"\n",
    "TRANSCRIPTION_MODEL_NAME = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "# Visualization\n",
    "MAX_SENTENCE_CHARACTERS = 128\n",
    "\n",
    "# Devices, dtypes\n",
    "device_name = torch.device(\"cuda\") if torch.cuda.is_available() else (\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "device = torch.device(device_name)\n",
    "torch_dtype = torch.bfloat16\n",
    "\n",
    "# --------------------- Transcription Pipeline -------------------------------\n",
    "\n",
    "transcription_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    TRANSCRIPTION_MODEL_NAME, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n",
    ")\n",
    "transcription_model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(TRANSCRIPTION_MODEL_NAME)\n",
    "\n",
    "transcription_pipeline = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=transcription_model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    chunk_length_s = LENGTH_IN_SEC,\n",
    "    batch_size = 1,  # batch size for inference - set based on your device\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Global flag to control transcription\n",
    "is_transcribing = True\n",
    "\n",
    "def producer():\n",
    "    global is_transcribing\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(\n",
    "        format=pyaudio.paInt16,\n",
    "        channels=NB_CHANNELS,\n",
    "        rate=SAMPLE_RATE,\n",
    "        input=True,\n",
    "        frames_per_buffer=CHUNK,\n",
    "    )\n",
    "\n",
    "    while is_transcribing:\n",
    "        audio_data = b\"\"\n",
    "        for _ in range(STEP_IN_SEC):\n",
    "            chunk = stream.read(SAMPLE_RATE)\n",
    "            audio_data += chunk\n",
    "\n",
    "        audio_queue.put(audio_data)\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "def consumer():\n",
    "    global is_transcribing\n",
    "    while is_transcribing:\n",
    "        if length_queue.qsize() >= LENGTH_IN_SEC:\n",
    "            length_queue._queue.clear()\n",
    "\n",
    "        audio_data = audio_queue.get()\n",
    "        length_queue.put(audio_data)\n",
    "\n",
    "        audio_data_to_process = b\"\"\n",
    "        for i in range(length_queue.qsize()):\n",
    "            audio_data_to_process += length_queue._queue[i]\n",
    "\n",
    "        audio_data_array = np.frombuffer(audio_data_to_process, np.int16).astype(np.float32) / 255.0\n",
    "\n",
    "        transcription_out = transcription_pipeline({\"array\":audio_data_array, \"sampling_rate\":SAMPLE_RATE}, \n",
    "                                             return_timestamps=True, \n",
    "                                             generate_kwargs={\"language\": \"english\", \"return_timestamps\": True, \"max_new_tokens\": MAX_SENTENCE_CHARACTERS})\n",
    "\n",
    "        # yield f\"data: {transcription_out[\"text\"]}\\n\\n\"\n",
    "        print(transcription_out[\"text\"], end='\\r', flush=True)\n",
    "        audio_queue.task_done()\n",
    "\n",
    "\n",
    "producer = threading.Thread(target=producer)\n",
    "producer.start()\n",
    "\n",
    "consumer = threading.Thread(target=consumer)\n",
    "consumer.start()\n",
    "\n",
    "try:\n",
    "    producer.join()\n",
    "    consumer.join()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have passed more than 3000 mel input features (> 30 seconds) which automatically enables long-form generation which requires the model to predict timestamp tokens. Please either pass `return_timestamps=True` or make sure to pass no more than 3000 mel input features.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m sample \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 32\u001b[0m result \u001b[38;5;241m=\u001b[39m pipe(sample)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/automatic_speech_recognition.py:284\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    223\u001b[0m     inputs: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    225\u001b[0m ):\n\u001b[1;32m    226\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m    documentation for more information.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/base.py:1260\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[0;32m-> 1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1261\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1262\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1263\u001b[0m                 [inputs], num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1264\u001b[0m             )\n\u001b[1;32m   1265\u001b[0m         )\n\u001b[1;32m   1266\u001b[0m     )\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/base.py:1175\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1174\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1175\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1176\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/automatic_speech_recognition.py:512\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline._forward\u001b[0;34m(self, model_inputs, return_timestamps, **generate_kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    510\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 512\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    513\u001b[0m     inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    514\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs,\n\u001b[1;32m    516\u001b[0m )\n\u001b[1;32m    517\u001b[0m \u001b[38;5;66;03m# whisper longform generation stores timestamps in \"segments\"\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_timestamps \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq2seq_whisper\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:520\u001b[0m, in \u001b[0;36mWhisperGenerationMixin.generate\u001b[0;34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, return_token_timestamps, return_segments, return_dict_in_generate, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# 3. Make sure generation config is correctly set\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;66;03m# Make sure the generation config is correctly set depending on whether timestamps are to be returned or not\u001b[39;00m\n\u001b[1;32m    514\u001b[0m return_dict_in_generate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_return_outputs(\n\u001b[1;32m    515\u001b[0m     return_dict_in_generate\u001b[38;5;241m=\u001b[39mreturn_dict_in_generate,\n\u001b[1;32m    516\u001b[0m     return_token_timestamps\u001b[38;5;241m=\u001b[39mreturn_token_timestamps,\n\u001b[1;32m    517\u001b[0m     logprob_threshold\u001b[38;5;241m=\u001b[39mlogprob_threshold,\n\u001b[1;32m    518\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m    519\u001b[0m )\n\u001b[0;32m--> 520\u001b[0m timestamp_begin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_return_timestamps(\n\u001b[1;32m    521\u001b[0m     return_timestamps\u001b[38;5;241m=\u001b[39mreturn_timestamps, is_shortform\u001b[38;5;241m=\u001b[39mis_shortform, generation_config\u001b[38;5;241m=\u001b[39mgeneration_config\n\u001b[1;32m    522\u001b[0m )\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_language_and_task(\n\u001b[1;32m    524\u001b[0m     language\u001b[38;5;241m=\u001b[39mlanguage, task\u001b[38;5;241m=\u001b[39mtask, is_multilingual\u001b[38;5;241m=\u001b[39mis_multilingual, generation_config\u001b[38;5;241m=\u001b[39mgeneration_config\n\u001b[1;32m    525\u001b[0m )\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_num_frames(\n\u001b[1;32m    527\u001b[0m     return_token_timestamps\u001b[38;5;241m=\u001b[39mreturn_token_timestamps, generation_config\u001b[38;5;241m=\u001b[39mgeneration_config, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    528\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:1188\u001b[0m, in \u001b[0;36mWhisperGenerationMixin._set_return_timestamps\u001b[0;34m(self, return_timestamps, is_shortform, generation_config)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_shortform:\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_timestamps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m-> 1188\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1189\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed more than 3000 mel input features (> 30 seconds) which automatically enables long-form generation which \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1190\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires the model to predict timestamp tokens. Please either pass `return_timestamps=True` or make sure to pass no more than 3000 mel input features.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1191\u001b[0m         )\n\u001b[1;32m   1193\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetting `return_timestamps=True` for long-form generation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1194\u001b[0m     return_timestamps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: You have passed more than 3000 mel input features (> 30 seconds) which automatically enables long-form generation which requires the model to predict timestamp tokens. Please either pass `return_timestamps=True` or make sure to pass no more than 3000 mel input features."
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "# from datasets import load_dataset\n",
    "\n",
    "\n",
    "# device = torch.device(\"mps\")\n",
    "# torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "# model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "#     model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    "# )\n",
    "# model.to(device)\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "#    return_timestamps = True\n",
    ")\n",
    "\n",
    "# dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "# sample = dataset[0][\"audio\"]\n",
    "# del sample[\"path\"]\n",
    "\n",
    "# result = pipe(sample)\n",
    "# print(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline({\"array\":sample[\"array\"], }, \n",
    "                                             return_timestamps=True, \n",
    "                                             generate_kwargs={\"language\": \"english\", \"return_timestamps\": True, \n",
    "                                                              \"max_new_tokens\": 128})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
