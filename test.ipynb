{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai/whisper-large-v3-turbo loaded\n"
     ]
    }
   ],
   "source": [
    "import queue\n",
    "import re\n",
    "import pyaudio\n",
    "import asyncio\n",
    "import torch \n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import numpy as np\n",
    "import queue\n",
    "import re\n",
    "import threading\n",
    "import time\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "\n",
    "TRANSCRIPTION_MODEL_NAME = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "# Audio settings\n",
    "STEP_IN_SEC: int = 1    # We'll increase the processable audio data by this\n",
    "LENGTH_IN_SEC: int = 10    # We'll process this amount of audio data together maximum\n",
    "NB_CHANNELS = 1\n",
    "RATE = 16000\n",
    "CHUNK = RATE\n",
    "\n",
    "# Whisper settings\n",
    "WHISPER_LANGUAGE = \"en\"\n",
    "WHISPER_THREADS = 4\n",
    "\n",
    "# Visualization (expected max number of characters for LENGHT_IN_SEC audio)\n",
    "MAX_SENTENCE_CHARACTERS = 128\n",
    "\n",
    "\n",
    "device_name = torch.device(\"cuda\") if torch.cuda.is_available() else (\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "device = torch.device(device_name)\n",
    "torch_dtype = torch.bfloat16\n",
    "\n",
    "transcription_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    TRANSCRIPTION_MODEL_NAME, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n",
    ")\n",
    "transcription_model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(TRANSCRIPTION_MODEL_NAME)\n",
    "\n",
    "transcription_pipeline = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=transcription_model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    chunk_length_s = min(LENGTH_IN_SEC, 30), # it uses sliding window and other protocol\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(f\"{TRANSCRIPTION_MODEL_NAME} loaded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Yeah I could do this config with argparse, but I won't...\n",
    "# This queue holds all the 1-second audio chunks\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# This queue holds all the chunks that will be processed together\n",
    "# If the chunk is filled to the max, it will be emptied\n",
    "length_queue = queue.Queue(maxsize=LENGTH_IN_SEC)\n",
    "\n",
    "\n",
    "def producer_thread():\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(\n",
    "        format=pyaudio.paInt16,\n",
    "        channels=NB_CHANNELS,\n",
    "        rate=RATE,\n",
    "        input=True,\n",
    "        frames_per_buffer=CHUNK,    # 1 second of audio\n",
    "    )\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Microphone initialized, recording started...\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"TRANSCRIPTION\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    while True:\n",
    "        audio_data = b\"\"\n",
    "        for _ in range(STEP_IN_SEC):\n",
    "            chunk = stream.read(RATE)    # Read 1 second of audio data\n",
    "            audio_data += chunk\n",
    "\n",
    "        audio_queue.put(audio_data)    # Put the 5-second audio data into the queue\n",
    "\n",
    "\n",
    "# Thread which gets items from the queue and prints its length\n",
    "def consumer_thread(stats):\n",
    "    while True:\n",
    "        if length_queue.qsize() >= LENGTH_IN_SEC:\n",
    "            with length_queue.mutex:\n",
    "                length_queue.queue.clear()\n",
    "                print()\n",
    "\n",
    "        audio_data = audio_queue.get()\n",
    "        transcription_start_time = time.time()\n",
    "        length_queue.put(audio_data)\n",
    "\n",
    "        # Concatenate audio data in the lenght_queue\n",
    "        audio_data_to_process = b\"\"\n",
    "        for i in range(length_queue.qsize()):\n",
    "            # We index it so it won't get removed\n",
    "            audio_data_to_process += length_queue.queue[i]\n",
    "\n",
    "        # convert the bytes data toa  numpy array\n",
    "        audio_data_array: np.ndarray = np.frombuffer(audio_data_to_process, np.int16).astype(np.float32) / 255.0\n",
    "        # audio_data_array = np.expand_dims(audio_data_array, axis=0)\n",
    "\n",
    "        transcription = transcription_pipeline({\"array\": audio_data_array, \"sampling_rate\": RATE},\n",
    "        return_timestamps=True,\n",
    "        generate_kwargs={\"language\": \"english\", \"return_timestamps\": True, \"max_new_tokens\": 128})[\"text\"]\n",
    "\n",
    "        transcription_end_time = time.time()\n",
    "\n",
    "        # remove anything from the text which is between () or [] --> these are non-verbal background noises/music/etc.\n",
    "        transcription = re.sub(r\"\\[.*\\]\", \"\", transcription)\n",
    "        transcription = re.sub(r\"\\(.*\\)\", \"\", transcription)\n",
    "        # We do this for the more clean visualization (when the next transcription we print would be shorter then the one we printed)\n",
    "        transcription = transcription.ljust(MAX_SENTENCE_CHARACTERS, \" \")\n",
    "\n",
    "        transcription_postprocessing_end_time = time.time()\n",
    "\n",
    "        print(transcription, end='\\r', flush=True)\n",
    "\n",
    "        audio_queue.task_done()\n",
    "\n",
    "        overall_elapsed_time = transcription_postprocessing_end_time - transcription_start_time\n",
    "        transcription_elapsed_time = transcription_end_time - transcription_start_time\n",
    "        postprocessing_elapsed_time = transcription_postprocessing_end_time - transcription_end_time\n",
    "        stats[\"overall\"].append(overall_elapsed_time)\n",
    "        stats[\"transcription\"].append(transcription_elapsed_time)\n",
    "        stats[\"postprocessing\"].append(postprocessing_elapsed_time)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    stats: Dict[str, List[float]] = {\"overall\": [], \"transcription\": [], \"postprocessing\": []}\n",
    "\n",
    "    producer = threading.Thread(target=producer_thread)\n",
    "    producer.start()\n",
    "\n",
    "    consumer = threading.Thread(target=consumer_thread, args=(stats,))\n",
    "    consumer.start()\n",
    "\n",
    "    try:\n",
    "        producer.join()\n",
    "        consumer.join()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Exiting...\")\n",
    "        # print out the statistics\n",
    "        print(\"Number of processed chunks: \", len(stats[\"overall\"]))\n",
    "        print(f\"Overall time: avg: {np.mean(stats['overall']):.4f}s, std: {np.std(stats['overall']):.4f}s\")\n",
    "        print(\n",
    "            f\"Transcription time: avg: {np.mean(stats['transcription']):.4f}s, std: {np.std(stats['transcription']):.4f}s\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Postprocessing time: avg: {np.mean(stats['postprocessing']):.4f}s, std: {np.std(stats['postprocessing']):.4f}s\"\n",
    "        )\n",
    "        # We need to add the step_in_sec to the latency as we need to wait for that chunk of audio\n",
    "        print(f\"The average latency is {np.mean(stats['overall'])+STEP_IN_SEC:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
